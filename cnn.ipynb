{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import truncnorm as tn\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_layer(m_input,size_in,size_out,k_size_w,k_size_h,conv_stride,pool_k_size,pool_stride_size,trainable_vars,name,num):\n",
    "    sdev = np.power(2.0/(k_size_w*k_size_h*size_in),0.5)\n",
    "    print(\"sdev\"+name+num+\": \",sdev)\n",
    "    with tf.name_scope(name+num):\n",
    "        w = tf.Variable(tf.truncated_normal([k_size_w,k_size_h,size_in,size_out],stddev=sdev,dtype=tf.float16),\n",
    "                        dtype=tf.float16,\n",
    "                        trainable=trainable_vars,\n",
    "                        name=\"w{}\".format(num))\n",
    "        b = tf.Variable(tf.constant(0.0,shape=[size_out],dtype=tf.float16),\n",
    "                        dtype=tf.float16,\n",
    "                        trainable=trainable_vars,\n",
    "                        name=\"b{}\".format(num))\n",
    "        conv = tf.nn.conv2d(m_input,w,strides=[1,conv_stride,conv_stride,1],padding=\"SAME\")\n",
    "        act = tf.nn.leaky_relu((conv+b),alpha=0.1)\n",
    "        tf.summary.histogram(\"weights\",w)\n",
    "        tf.summary.histogram(\"biases\",b)\n",
    "        tf.summary.histogram(\"act\",act)\n",
    "        return tf.nn.max_pool(act,ksize=[1,pool_k_size,pool_k_size,1],strides=[1,pool_stride_size,pool_stride_size,1],padding='SAME')\n",
    "\n",
    "\n",
    "def fc_layer(m_input,size_in,size_out,trainable_vars,name,num):\n",
    "    sdev = np.power(2.0/(size_in*size_out),0.5)\n",
    "    print(\"sdev\"+name+num+\": \",sdev)\n",
    "    with tf.name_scope(name+num):\n",
    "        w = tf.Variable(tf.truncated_normal([size_in, size_out],stddev=sdev,dtype=tf.float16),\n",
    "                        dtype=tf.float16,\n",
    "                        trainable=trainable_vars,\n",
    "                        name=\"w{}\".format(num))\n",
    "        b = tf.Variable(tf.constant(0.0,shape=[size_out],dtype=tf.float16),\n",
    "                        dtype=tf.float16,\n",
    "                        trainable=trainable_vars,\n",
    "                        name=\"b{}\".format(num))\n",
    "        z = tf.matmul(m_input,w)\n",
    "        act = tf.nn.leaky_relu(z+b,alpha=0.1,name=(\"act\"+num))\n",
    "        tf.summary.histogram(\"weights\",w)\n",
    "        tf.summary.histogram(\"biases\",b)\n",
    "        tf.summary.histogram(\"act\",act)\n",
    "        return act\n",
    "        \n",
    "    \n",
    "    \n",
    "def get_place_holders():\n",
    "    a = tf.get_default_graph().get_tensor_by_name(\"place_holder/x1:0\")\n",
    "    b = tf.get_default_graph().get_tensor_by_name(\"place_holder/y:0\")\n",
    "    c = tf.get_default_graph().get_tensor_by_name(\"place_holder/next_state:0\")\n",
    "    d = tf.get_default_graph().get_tensor_by_name(\"place_holder/qnext:0\")\n",
    "    return a,b,c,d\n",
    "\n",
    "\n",
    "def build_graph(name,net_in,conv_count,fc_count,conv_feats,fc_feats,conv_k_size,conv_stride,trainable_vars):\n",
    "    with tf.name_scope(name):\n",
    "        conv_name=\"conv\"\n",
    "        fcs_name=\"FC\"\n",
    "        conv_feats[0] = 4\n",
    "        fc_feats[0] = 384\n",
    "        with tf.name_scope(\"Convolution_Layers\"):\n",
    "            convs = []\n",
    "            convs.append(net_in)\n",
    "            p = 0\n",
    "            for i in range(0,conv_count-1):\n",
    "                convs.append(conv_layer(convs[i],\n",
    "                                        conv_feats[i],conv_feats[i+1],\n",
    "                                        conv_k_size[p],conv_k_size[p],\n",
    "                                        conv_stride[p],\n",
    "                                        2,2,trainable_vars,\n",
    "                                        conv_name,str(i+1)))\n",
    "                p = p+1\n",
    "            \n",
    "            flatten = tf.reshape(convs[conv_count-1],[-1,fc_feats[0]])\n",
    "            \n",
    "        with tf.name_scope(\"Dense_Layers\"):\n",
    "            fcs = []\n",
    "            fcs.append(flatten)\n",
    "            for i in range(0,fc_count-1):\n",
    "                fcs.append(fc_layer(fcs[i],\n",
    "                                    fc_feats[i],fc_feats[i+1],\n",
    "                                    trainable_vars,fcs_name,str(i+1)))\n",
    "            output_layer = fcs[len(fcs)-1]\n",
    "    return output_layer\n",
    "\n",
    "def parse_fn(seq):\n",
    "    fmt = {\n",
    "        \"img1\": tf.FixedLenFeature([110,84,4],tf.int64,tf.zeros(shape=[110,84,4])),\n",
    "        \"a\": tf.FixedLenFeature([1],tf.int64,-1),\n",
    "        \"r\": tf.FixedLenFeature([1],tf.int64,-1),\n",
    "        \"img2\": tf.FixedLenFeature([110,84,4],tf.int64,tf.zeros(shape=[110,84,4]))        \n",
    "    }\n",
    "    \n",
    "    parsed = tf.parse_single_example(seq,fmt)\n",
    "    img1 = parsed[\"img1\"]\n",
    "    img2 = parsed[\"img2\"]\n",
    "    a = parsed[\"a\"]\n",
    "    r = parsed[\"r\"]\n",
    "    img1 = tf.map_fn(lambda frame: tf.image.per_image_standardization(frame),tf.expand_dims(tf.reshape(img1,shape=[4,110,84]),axis=3))\n",
    "    img2 = tf.map_fn(lambda frame: tf.image.per_image_standardization(frame),tf.expand_dims(tf.reshape(img2,shape=[4,110,84]),axis=3))\n",
    "    img1 = tf.cast(tf.reshape(img1,shape=[110,84,4]),tf.float16)\n",
    "    img2 = tf.cast(tf.reshape(img2,shape=[110,84,4]),tf.float16)\n",
    "    return img1,a,r,img2\n",
    "\n",
    "def build_train_data_pipeline(filenames,batchsize):\n",
    "    with tf.name_scope(\"Train_Data_Pipeline\"):\n",
    "        files = tf.data.Dataset.list_files(filenames)\n",
    "        dataset = files.apply(tf.contrib.data.parallel_interleave(\n",
    "            lambda filename: tf.data.Dataset.from_tensors(filename),\n",
    "            cycle_length=4,\n",
    "            prefetch_input_elements=1))\n",
    "        dataset = dataset.shuffle(buffer_size=25)\n",
    "        dataset = dataset.map(parse_fn,num_parallel_calls=2)\n",
    "        dataset = dataset.batch(batchsize).prefetch(2)\n",
    "    return dataset\n",
    "        \n",
    "        \n",
    "\n",
    "def build_train_queue(batch_size):\n",
    "    with tf.name_scope(\"TrainQueue\"):\n",
    "        q = tf.FIFOQueue(capacity=25,\n",
    "                         dtypes= (tf.float16,tf.uint8,tf.float16,tf.float16),\n",
    "                         shapes= (tf.TensorShape([batch_size,110,84,4]),\n",
    "                                  tf.TensorShape([batch_size,1]),\n",
    "                                  tf.TensorShape([batch_size,1]),\n",
    "                                  tf.TensorShape([batch_size,110,84,4])),\n",
    "                         name=\"tq\",shared_name=\"train_queue\")\n",
    "    return q\n",
    "\n",
    "def build_update_infer_weights_op(conv_name,fc_name,conv_count,fc_count):\n",
    "    num_conv = conv_count\n",
    "    num_fc = fc_count\n",
    "    \n",
    "    def get_tensor(name):\n",
    "        return tf.get_default_graph().get_tensor_by_name(name)\n",
    "    \n",
    "    infer_conv_w = [get_tensor(\"Inference/Convolution_Layers/{}{}/w{}:0\".format(conv_name,i,i)) for i in range(1,num_conv)]\n",
    "    infer_conv_b = [get_tensor(\"Inference/Convolution_Layers/{}{}/b{}:0\".format(conv_name,i,i)) for i in range(1,num_conv)]\n",
    "    infer_fc_w = [get_tensor(\"Inference/Dense_Layers/{}{}/w{}:0\".format(fc_name,i,i)) for i in range(1,num_fc)]\n",
    "    infer_fc_b = [get_tensor(\"Inference/Dense_Layers/{}{}/b{}:0\".format(fc_name,i,i)) for i in range(1,num_fc)]\n",
    "    \n",
    "    \n",
    "    \n",
    "    train_conv_w = [get_tensor(\"Train/Convolution_Layers/{}{}/w{}:0\".format(conv_name,i,i)) for i in range(1,num_conv)]\n",
    "    train_conv_b = [get_tensor(\"Train/Convolution_Layers/{}{}/b{}:0\".format(conv_name,i,i)) for i in range(1,num_conv)]\n",
    "    train_fc_w = [get_tensor(\"Train/Dense_Layers/{}{}/w{}:0\".format(fc_name,i,i)) for i in range(1,num_fc)]\n",
    "    train_fc_b = [get_tensor(\"Train/Dense_Layers/{}{}/b{}:0\".format(fc_name,i,i)) for i in range(1,num_fc)]\n",
    "\n",
    "    assign_ops_conv_w = [tf.assign(a,b) for a,b in zip(infer_conv_w,train_conv_w)]\n",
    "    assign_ops_conv_b = [tf.assign(a,b) for a,b in zip(infer_conv_b,train_conv_b)]\n",
    "    assign_ops_fc_w = [tf.assign(a,b) for a,b in zip(infer_fc_w,train_fc_w)]\n",
    "    assign_ops_fc_b = [tf.assign(a,b) for a,b in zip(infer_fc_b,train_fc_b)]\n",
    "    return [assign_ops_conv_w,assign_ops_conv_b,assign_ops_fc_w,assign_ops_fc_b]\n",
    "\n",
    "def start_server():\n",
    "    server = tf.train.Server.create_local_server()\n",
    "    print(server.target)\n",
    "    return server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(learning_rate,gamma,batch_size,conv_count,fc_count,conv_feats,fc_feats,conv_k_size,conv_stride,LOGDIR):\n",
    "    if (len(conv_feats) != conv_count):\n",
    "        return\n",
    "    \n",
    "    tf.reset_default_graph()\n",
    "    \n",
    "    with tf.name_scope(\"infer_place_holder\"):\n",
    "        x1 = tf.placeholder_with_default(tf.cast(tf.constant(0.1,shape=[1,110,84,4]),tf.float16),shape=[None,110,84,4],name=\"x1\")\n",
    "        tr = tf.placeholder(tf.bool,name=\"train_bool\")\n",
    "    with tf.name_scope(\"train_place_holder\"):\n",
    "        seq = tf.placeholder(tf.float16,name=\"seq\")\n",
    "    \n",
    " \n",
    "    #dataset = build_train_data_pipeline(\"seq/test.tfrecord\",batch_size)\n",
    "    #dat_iter = dataset.make_one_shot_iterator()\n",
    "    #train_img,a,b,c = dat_iter.get_next()\n",
    "    train_q = build_train_queue(batch_size)\n",
    "    enqueue_op = train_q.enqueue(seq,name=\"train_enqueue\")\n",
    "    #dequeue_op = train_q.dequeue(name=\"train_dequeue\")\n",
    "    img1,a,r,img2 = train_q.dequeue(name=\"train_dequeue\")\n",
    "    tf.summary.image(\"image_s1\",img1,max_outputs=4)\n",
    "    tf.summary.image(\"image_s2\",img2,max_outputs=4)\n",
    "    \n",
    "    \n",
    "    condition = tf.equal(tf.reduce_max(x1),tf.cast(tf.constant(0.1),tf.float16))\n",
    "    infer_img = tf.cond(condition,lambda: img2,lambda: x1,name=\"train_conditional\")\n",
    "    \n",
    "    \n",
    "    infer_output = build_graph(\"Inference\",infer_img,conv_count,fc_count,conv_feats,fc_feats,conv_k_size,conv_stride,False)\n",
    "    #train_infer_output = build_graph(\"Train_Inference\",dequeue_op[0],conv_count,fc_count,conv_feats,fc_feats,conv_k_size,conv_stride)\n",
    "    train_output = build_graph(\"Train\",img1,conv_count,fc_count,conv_feats,fc_feats,conv_k_size,conv_stride,True)\n",
    "\n",
    "    Qnext = tf.reduce_max(infer_output,name=\"Qnext\")\n",
    "    gamma_seq = tf.tile(gamma,[batch_size])\n",
    "    y = tf.add(r,tf.multiply(gamma,Qnext),name=\"y\")\n",
    "    \n",
    "    with tf.name_scope(\"Trainer\"):\n",
    "        loss = tf.reduce_sum(tf.pow(y-train_output,2))\n",
    "        tf.summary.scalar(\"loss\",loss)\n",
    "        train = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss,name=\"trainer\")\n",
    "\n",
    "    with tf.name_scope(\"weight_update_ops\"):\n",
    "        ops = build_update_infer_weights_op(\"conv\",\"FC\",conv_count,fc_count)\n",
    "    \n",
    "    Qnext_val = tf.reduce_max(infer_output,name=\"Qnext_val\")\n",
    "    action = tf.argmax(infer_output,axis=1,name=\"action\")\n",
    "    \n",
    "    config = tf.ConfigProto()\n",
    "    config.gpu_options.allow_growth=True\n",
    "    \n",
    "    \n",
    "    summ = tf.summary.merge_all()\n",
    "    writer = tf.summary.FileWriter(LOGDIR)\n",
    "\n",
    "    sess = tf.InteractiveSession(config=config)\n",
    "    #sess = tf.InteractiveSession()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    summ = tf.summary.merge_all()\n",
    "    writer = tf.summary.FileWriter(LOGDIR)\n",
    "    return sess,writer,summ,x1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sdevconv1:  0.08838834764831845\n",
      "sdevconv2:  0.08838834764831845\n",
      "sdevFC1:  0.006454972243679028\n",
      "sdevFC2:  0.0565685424949238\n",
      "sdevconv1:  0.08838834764831845\n",
      "sdevconv2:  0.08838834764831845\n",
      "sdevFC1:  0.006454972243679028\n",
      "sdevFC2:  0.0565685424949238\n"
     ]
    }
   ],
   "source": [
    "conv_k_size = [8,4]\n",
    "conv_stride = [4,2]\n",
    "conv = [0,16,32]\n",
    "fclyr = [0,125,5]\n",
    "conv_count = len(conv)\n",
    "fc_count = len(fclyr)\n",
    "learning_rate = 1e-4\n",
    "gamma = np.array([.9]).astype(np.float16)\n",
    "batch_size = 10\n",
    "LOGDIR = r\"c:\\Users\\devar\\Documents\\EngProj\\SSPlayer\\log\"\n",
    "sess,writer,summ,place_holders= create_model(learning_rate,gamma,batch_size,conv_count,fc_count,conv,fclyr,conv_k_size,conv_stride,LOGDIR)\n",
    "\n",
    "writer.add_graph(sess.graph)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
