{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import truncnorm as tn\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_layer(m_input,size_in,size_out,k_size_w,k_size_h,conv_stride,pool_k_size,pool_stride_size,name,num):\n",
    "    sdev = np.power(2.0/(k_size_w*k_size_h*size_in),0.5)\n",
    "    print(\"sdev\"+name+num+\": \",sdev)\n",
    "    with tf.name_scope(name+num):\n",
    "        w = tf.Variable(tf.truncated_normal([k_size_w,k_size_h,size_in,size_out],stddev=sdev,dtype=tf.float16),dtype=tf.float16,name=\"w{}\".format(num))\n",
    "        b = tf.Variable(tf.constant(0.0,shape=[size_out],dtype=tf.float16),dtype=tf.float16,name=\"b{}\".format(num))\n",
    "        conv = tf.nn.conv2d(m_input,w,strides=[1,conv_stride,conv_stride,1],padding=\"SAME\")\n",
    "        act = tf.nn.leaky_relu((conv+b),alpha=0.1)\n",
    "        tf.summary.histogram(\"weights\",w)\n",
    "        tf.summary.histogram(\"biases\",b)\n",
    "        tf.summary.histogram(\"act\",act)\n",
    "        return tf.nn.max_pool(act,ksize=[1,pool_k_size,pool_k_size,1],strides=[1,pool_stride_size,pool_stride_size,1],padding='SAME')\n",
    "\n",
    "\n",
    "def fc_layer(m_input,size_in,size_out,name,num):\n",
    "    sdev = np.power(2.0/(size_in*size_out),0.5)\n",
    "    print(\"sdev\"+name+num+\": \",sdev)\n",
    "    with tf.name_scope(name+num):\n",
    "        w = tf.Variable(tf.truncated_normal([size_in, size_out],stddev=sdev,dtype=tf.float16),dtype=tf.float16,name=\"w{}\".format(num))\n",
    "        b = tf.Variable(tf.constant(0.0,shape=[size_out],dtype=tf.float16),dtype=tf.float16,name=\"b{}\".format(num))\n",
    "        z = tf.matmul(m_input,w)\n",
    "        act = tf.nn.leaky_relu(z+b,alpha=0.1,name=(\"act\"+num))\n",
    "        tf.summary.histogram(\"weights\",w)\n",
    "        tf.summary.histogram(\"biases\",b)\n",
    "        tf.summary.histogram(\"act\",act)\n",
    "        return act\n",
    "        \n",
    "    \n",
    "    \n",
    "def get_place_holders():\n",
    "    a = tf.get_default_graph().get_tensor_by_name(\"place_holder/x1:0\")\n",
    "    b = tf.get_default_graph().get_tensor_by_name(\"place_holder/y:0\")\n",
    "    c = tf.get_default_graph().get_tensor_by_name(\"place_holder/next_state:0\")\n",
    "    d = tf.get_default_graph().get_tensor_by_name(\"place_holder/qnext:0\")\n",
    "    return a,b,c,d\n",
    "\n",
    "\n",
    "def build_graph(name,net_in,conv_count,fc_count,conv_feats,fc_feats,conv_k_size,conv_stride):\n",
    "    with tf.name_scope(name):\n",
    "        conv_name=\"conv\"\n",
    "        fcs_name=\"FC\"\n",
    "        conv_feats[0] = 4\n",
    "        fc_feats[0] = 384\n",
    "        with tf.name_scope(\"Convolution_Layer\"):\n",
    "            convs = []\n",
    "            convs.append(net_in)\n",
    "            p = 0\n",
    "            for i in range(0,conv_count-1):\n",
    "                convs.append(conv_layer(convs[i],conv_feats[i],conv_feats[i+1],conv_k_size[p],conv_k_size[p],conv_stride[p],2,2,conv_name,str(i+1)))\n",
    "                p = p+1\n",
    "            \n",
    "            flatten = tf.reshape(convs[conv_count-1],[-1,fc_feats[0]])\n",
    "            \n",
    "        with tf.name_scope(\"dense_layers\"):\n",
    "            fcs = []\n",
    "            fcs.append(flatten)\n",
    "            for i in range(0,fc_count-1):\n",
    "                fcs.append(fc_layer(fcs[i],fc_feats[i],fc_feats[i+1],fcs_name,str(i+1)))\n",
    "            output_layer = fcs[len(fcs)-1]\n",
    "    \n",
    "    return output_layer\n",
    "\n",
    "def parse_fn(seq):\n",
    "    fmt = {\n",
    "        \"img1\": tf.FixedLenFeature([110,84,4],tf.int64,tf.zeros(shape=[110,84,4])),\n",
    "        \"a\": tf.FixedLenFeature([1],tf.int64,-1),\n",
    "        \"r\": tf.FixedLenFeature([1],tf.int64,-1),\n",
    "        \"img2\": tf.FixedLenFeature([110,84,4],tf.int64,tf.zeros(shape=[110,84,4]))        \n",
    "    }\n",
    "    \n",
    "    parsed = tf.parse_single_example(seq,fmt)\n",
    "    img1 = parsed[\"img1\"]\n",
    "    img2 = parsed[\"img2\"]\n",
    "    a = parsed[\"a\"]\n",
    "    r = parsed[\"r\"]\n",
    "    img1 = tf.map_fn(lambda frame: tf.image.per_image_standardization(frame),tf.expand_dims(tf.reshape(img1,shape=[4,110,84]),axis=3))\n",
    "    img2 = tf.map_fn(lambda frame: tf.image.per_image_standardization(frame),tf.expand_dims(tf.reshape(img2,shape=[4,110,84]),axis=3))\n",
    "    img1 = tf.cast(tf.reshape(img1,shape=[110,84,4]),tf.float16)\n",
    "    img2 = tf.cast(tf.reshape(img2,shape=[110,84,4]),tf.float16)\n",
    "    return img1,a,r,img2\n",
    "\n",
    "def build_train_data_pipeline(filenames,batchsize):\n",
    "    with tf.name_scope(\"Train_Data_Pipeline\"):\n",
    "        files = tf.data.Dataset.list_files(filenames)\n",
    "        dataset = files.apply(tf.contrib.data.parallel_interleave(lambda filename: tf.data.TFRecordDataset(filename)\n",
    "                                                                  ,cycle_length=4,prefetch_input_elements=1))\n",
    "        dataset = dataset.shuffle(buffer_size=25)\n",
    "        dataset = dataset.map(parse_fn,num_parallel_calls=2)\n",
    "        dataset = dataset.batch(batchsize).prefetch(2)\n",
    "    return dataset\n",
    "        \n",
    "        \n",
    "\n",
    "def build_train_queue():\n",
    "    with tf.name_scope(\"TrainQueue\"):\n",
    "        q = tf.FIFOQueue(capacity=25,\n",
    "                         dtypes= (tf.float16,tf.uint8,tf.float16,tf.float16),\n",
    "                         shapes= (tf.TensorShape([1,110,84,4]),\n",
    "                                  tf.TensorShape([1,1]),\n",
    "                                  tf.TensorShape([1,1]),\n",
    "                                  tf.TensorShape([1,110,84,4])),\n",
    "                         name=\"trian_queue\")\n",
    "    return q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(learning_rate,batch_size,conv_count,fc_count,conv_feats,fc_feats,conv_k_size,conv_stride,LOGDIR):\n",
    "    if (len(conv_feats) != conv_count):\n",
    "        return\n",
    "    \n",
    "    tf.reset_default_graph()\n",
    "    \n",
    "    with tf.name_scope(\"place_holder\"):\n",
    "        x1 = tf.placeholder(tf.float16,shape=[None,110,84,4],name=\"x1\")\n",
    "        y = tf.placeholder(tf.float16,shape=[None,4],name=\"y\")\n",
    "        next_state = tf.placeholder(tf.bool,name=\"next_state\")\n",
    "        Qnext = tf.placeholder(tf.float16,shape=[None,1],name=\"qnext\")\n",
    "    \n",
    " \n",
    "    dataset = build_train_data_pipeline(\"seq/test.tfrecord\",batch_size)\n",
    "    dat_iter = dataset.make_one_shot_iterator()\n",
    "    train_img,a,b,c = dat_iter.get_next()\n",
    "    #train_q = build_train_queue()\n",
    "    #enqueue_op = train_q.enqueue(x1,name=\"train_enqueue\")\n",
    "    #dequeue_op = train_q.dequeue(name=\"train_dequeue\")\n",
    "    tf.summary.image(\"image\",x1,max_outputs=4)\n",
    "    \n",
    "    \n",
    "    infer_output = build_graph(\"Inference\",x1,conv_count,fc_count,conv_feats,fc_feats,conv_k_size,conv_stride)\n",
    "    train_output = build_graph(\"Train\",train_img,conv_count,fc_count,conv_feats,fc_feats,conv_k_size,conv_stride)\n",
    "    \n",
    "    \n",
    "    \n",
    "    with tf.name_scope(\"train\"):\n",
    "        loss = tf.reduce_sum(tf.pow(Qnext-train_output,2))\n",
    "        tf.summary.scalar(\"loss\",loss)\n",
    "        train = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss,name=\"trainer\")\n",
    "\n",
    "    \n",
    "    Qnext_val = tf.reduce_max(infer_output,name=\"Qnext_val\")\n",
    "    action = tf.argmax(infer_output,axis=1,name=\"action\")\n",
    "    \n",
    "    config = tf.ConfigProto()\n",
    "    config.gpu_options.allow_growth=True\n",
    "    \n",
    "    \n",
    "    summ = tf.summary.merge_all()\n",
    "    writer = tf.summary.FileWriter(LOGDIR)\n",
    "\n",
    "    sess = tf.InteractiveSession(config=config)\n",
    "    #sess = tf.InteractiveSession()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    summ = tf.summary.merge_all()\n",
    "    writer = tf.summary.FileWriter(LOGDIR)\n",
    "    return sess,writer,summ,[x1,y,next_state,Qnext]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sdevconv1:  0.08838834764831845\n",
      "sdevconv2:  0.08838834764831845\n",
      "sdevFC1:  0.006454972243679028\n",
      "sdevFC2:  0.0565685424949238\n",
      "sdevconv1:  0.08838834764831845\n",
      "sdevconv2:  0.08838834764831845\n",
      "sdevFC1:  0.006454972243679028\n",
      "sdevFC2:  0.0565685424949238\n"
     ]
    }
   ],
   "source": [
    "conv_k_size = [8,4]\n",
    "conv_stride = [4,2]\n",
    "conv = [0,16,32]\n",
    "fclyr = [0,125,5]\n",
    "conv_count = len(conv)\n",
    "fc_count = len(fclyr)\n",
    "learning_rate = 1e-4\n",
    "batch_size = 10\n",
    "LOGDIR = r\"c:\\Users\\Vishnu\\Documents\\EngProj\\SSPlayer\\log\"\n",
    "sess,writer,summ,place_holders= create_model(learning_rate,batch_size,conv_count,fc_count,conv,fclyr,conv_k_size,conv_stride,LOGDIR)\n",
    "\n",
    "writer.add_graph(sess.graph)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
